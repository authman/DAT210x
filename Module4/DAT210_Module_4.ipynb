{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\ADEKUNLE\\Documents\\GitHub\\DAT210x\\Module4\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assignment1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assignment1.py\n",
    "# %load assignment1.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import datetime\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "\n",
    "# Every 100 data samples, we save 1. If things run too\n",
    "# slow, try increasing this number. If things run too fast,\n",
    "# try decreasing it... =)\n",
    "reduce_factor = 100\n",
    "\n",
    "\n",
    "# Look pretty...\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Load up the scanned armadillo\n",
    "plyfile = PlyData.read('Datasets/stanford_armadillo.ply')\n",
    "armadillo = pd.DataFrame({\n",
    "  'x':plyfile['vertex']['z'][::reduce_factor],\n",
    "  'y':plyfile['vertex']['x'][::reduce_factor],\n",
    "  'z':plyfile['vertex']['y'][::reduce_factor]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "def do_PCA(armadillo):\n",
    "  #\n",
    "  # TODO: Write code to import the libraries required for PCA.\n",
    "  # Then, train your PCA on the armadillo dataframe. Finally,\n",
    "  # drop one dimension (reduce it down to 2D) and project the\n",
    "  # armadillo down to the 2D principal component feature space.\n",
    "  #\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca.fit(armadillo)\n",
    "    \n",
    "    armadillo_P = pca.transform(armadillo)\n",
    "  # NOTE: Be sure to RETURN your projected armadillo! \n",
    "  # (This projection is actually stored in a NumPy NDArray and\n",
    "  # not a Pandas dataframe, which is something Pandas does for\n",
    "  # you automatically. =)\n",
    "  #\n",
    "  # .. your code here ..\n",
    "    return armadillo_P\n",
    "\n",
    "\n",
    "def do_RandomizedPCA(armadillo):\n",
    "  #\n",
    "  # TODO: Write code to import the libraries required for\n",
    "  # RandomizedPCA. Then, train your RandomizedPCA on the armadillo\n",
    "  # dataframe. Finally, drop one dimension (reduce it down to 2D)\n",
    "  # and project the armadillo down to the 2D principal component\n",
    "  # feature space.\n",
    "    from sklearn.decomposition import RandomizedPCA\n",
    "    RandomizedPca = RandomizedPCA(n_components = 2)\n",
    "    RandomizedPca.fit(armadillo)\n",
    "    \n",
    "    armadillo_RP = RandomizedPca.transform(armadillo)\n",
    "  # NOTE: Be sure to RETURN your projected armadillo! \n",
    "  # (This projection is actually stored in a NumPy NDArray and\n",
    "  # not a Pandas dataframe, which is something Pandas does for\n",
    "  # you automatically. =)\n",
    "  #\n",
    "  # .. your code here ..\n",
    "    return armadillo_RP\n",
    "\n",
    "\n",
    "\n",
    "# Render the Original Armadillo\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d') #remember that whenever you are plotting 3D you add projection to the add_subplot\n",
    "ax.set_title('Armadillo 3D')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.scatter(armadillo.x, armadillo.y, armadillo.z, c='green', marker='.', alpha=0.75)\n",
    "\n",
    "\n",
    "\n",
    "# Time the execution of PCA 5000x\n",
    "t1 = datetime.datetime.now()\n",
    "for i in range(5000): pca = do_PCA(armadillo)\n",
    "time_delta = datetime.datetime.now() - t1\n",
    "\n",
    "# Render the newly transformed PCA armadillo!\n",
    "if not pca is None: #intuitively saying if pca = do_PCA(armadillo) is not None\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('PCA, build time: ' + str(time_delta))\n",
    "    ax.scatter(pca[:,0], pca[:,1], c='blue', marker='.', alpha=0.75)\n",
    "\n",
    "# Time the execution of rPCA 5000x\n",
    "t1 = datetime.datetime.now()\n",
    "for i in range(5000): rpca = do_RandomizedPCA(armadillo)\n",
    "time_delta = datetime.datetime.now() - t1\n",
    "\n",
    "# Render the newly transformed RandomizedPCA armadillo!\n",
    "if not rpca is None:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('RandomizedPCA, build time: ' + str(time_delta))\n",
    "    ax.scatter(rpca[:,0], rpca[:,1], c='red', marker='.', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assignment3.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import assignment2_helper as helper\n",
    "%matplotlib inline\n",
    "\n",
    "# Look pretty...\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Do * NOT * alter this line, until instructed!\n",
    "scaleFeatures = True\n",
    "\n",
    "\n",
    "# TODO: Load up the dataset and remove any and all\n",
    "# Rows that have a nan. You should be a pro at this\n",
    "# by now ;-)\n",
    "#\n",
    "# .. your code here ..\n",
    "kidneyData = pd.read_csv('Datasets/kidney_disease.csv')\n",
    "kidneyDataNull = kidneyData.dropna(axis = 0, how='any' ).reset_index(drop = True)\n",
    "\n",
    "# Create some color coded labels; the actual label feature\n",
    "# will be removed prior to executing PCA, since it's unsupervised.\n",
    "# You're only labeling by color so you can see the effects of PCA\n",
    "labels = ['red' if i=='ckd' else 'green' for i in kidneyDataNull.classification]\n",
    "\n",
    "\n",
    "# TODO: Use an indexer to select only the following columns:\n",
    "#       ['bgr','wc','rc']\n",
    "#\n",
    "# .. your code here ..\n",
    "\n",
    "selectMain = kidneyDataNull.loc[:, ['bgr', 'wc', 'rc']]\n",
    "\n",
    "# TODO: Print out and check your dataframe's dtypes. You'll probably\n",
    "# want to call 'exit()' after you print it out so you can stop the\n",
    "# program's execution.\n",
    "#\n",
    "print(selectMain.dtypes)\n",
    "\n",
    "\n",
    "# You can either take a look at the dataset webpage in the attribute info\n",
    "# section: https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease\n",
    "# or you can actually peek through the dataframe by printing a few rows.\n",
    "# What kind of data type should these three columns be? If Pandas didn't\n",
    "# properly detect and convert them to that data type for you, then use\n",
    "# an appropriate command to coerce these features into the right type.\n",
    "#\n",
    "# .. your code here ..\n",
    "\n",
    "selectMain = selectMain.apply(lambda s: pd.to_numeric(s))\n",
    "print(selectMain.dtypes)\n",
    "# TODO: PCA Operates based on variance. The variable with the greatest\n",
    "# variance will dominate. Go ahead and peek into your data using a\n",
    "# command that will check the variance of every feature in your dataset.\n",
    "\n",
    "print(selectMain.var())\n",
    "# Print out the results. Also print out the results of running .describe\n",
    "# on your dataset.\n",
    "#\n",
    "# Hint: If you don't see all three variables: 'bgr','wc' and 'rc', then\n",
    "# you probably didn't complete the previous step properly.\n",
    "#\n",
    "# .. your code here ..\n",
    "print(selectMain.describe())\n",
    "\n",
    "\n",
    "# TODO: This method assumes your dataframe is called df. If it isn't,\n",
    "# make the appropriate changes. Don't alter the code in scaleFeatures()\n",
    "# just yet though!\n",
    "#\n",
    "# .. your code adjustment here ..\n",
    "if scaleFeatures: selectMain = helper.scaleFeatures(selectMain)\n",
    "\n",
    "\n",
    "# TODO: Run PCA on your dataset and reduce it to 2 components\n",
    "# Ensure your PCA instance is saved in a variable called 'pca',\n",
    "# and that the results of your transformation are saved in 'T'.\n",
    "#\n",
    "# .. your code here ..\n",
    "    #import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(selectMain)\n",
    "T = pca.transform(selectMain)\n",
    "\n",
    "\n",
    "# Plot the transformed data as a scatter plot. Recall that transforming\n",
    "# the data will result in a NumPy NDArray. You can either use MatPlotLib\n",
    "# to graph it directly, or you can convert it to DataFrame and have pandas\n",
    "# do it for you.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.set_title('Unreduced Format of selectMain')\n",
    "ax.set_xlabel('bgr')\n",
    "ax.set_ylabel('rc')\n",
    "ax.set_zlabel('wc')\n",
    "ax.scatter(selectMain.bgr, selectMain.rc, selectMain.wc, c = 'green', marker = '.')\n",
    "#\n",
    "\n",
    "#plot reduced form of selectMain\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.set_title('PCA reduction of Selected Features of KidneyData')\n",
    "ax2.set_xlabel('Component 1')\n",
    "ax2.set_ylabel('Component 2')\n",
    "ax2.scatter(T[:, 0], T[:, 1], c = 'orange' , marker = 'o', alpha = 0.75)\n",
    "plt.show()\n",
    "# Since we've already demonstrated how to plot directly with MatPlotLib in\n",
    "# Module4/assignment1.py, this time we'll convert to a Pandas Dataframe.\n",
    "#\n",
    "# Since we transformed via PCA, we no longer have column names. We know we\n",
    "# are in P.C. space, so we'll just define the coordinates accordingly:\n",
    "ax = helper.drawVectors(T, pca.components_, selectMain.columns.values, plt, scaleFeatures)\n",
    "T = pd.DataFrame(T)\n",
    "T.columns = ['component1', 'component2']\n",
    "\n",
    "T.plot.scatter(x='component1', y='component2', marker='o', c=labels, alpha=0.75, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60180443,  0.49016625, -0.63053032],\n",
       "       [-0.45604187,  0.85904029,  0.23254161]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load assignment2_helper.py\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# A Note on SKLearn .transform() calls:\n",
    "#\n",
    "# Any time you transform your data, you lose the column header names.\n",
    "# This actually makes complete sense. There are essentially two types\n",
    "# of transformations,  those that change the scale of your features,\n",
    "# and those that change your features entire. Changing the scale would\n",
    "# be like changing centimeters to inches. Changing the features would\n",
    "# be like using PCA to reduce 300 columns to 30. In either case, the\n",
    "# original column's units have been altered or no longer exist, so it's\n",
    "# up to you to rename your columns after ANY transformation. Due to\n",
    "# this, SKLearn returns an NDArray from *transform() calls.\n",
    "\n",
    "def scaleFeatures(df):\n",
    "  # SKLearn has many different methods for doing transforming your\n",
    "  # features by scaling them (this is a type of pre-processing).\n",
    "  # RobustScaler, Normalizer, MinMaxScaler, MaxAbsScaler, StandardScaler...\n",
    "  # http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "  #\n",
    "  # However in order to be effective at PCA, there are a few requirements\n",
    "  # that must be met, and which will drive the selection of your scaler.\n",
    "  # PCA required your data is standardized -- in other words it's mean is\n",
    "  # equal to 0, and it has ~unit variance.\n",
    "  #\n",
    "  # SKLearn's regular Normalizer doesn't zero out the mean of your data,\n",
    "  # it only clamps it, so it's inappropriate to use here (depending on\n",
    "  # your data). MinMaxScaler and MaxAbsScaler both fail to set a unit\n",
    "  # variance, so you won't be using them either. RobustScaler can work,\n",
    "  # again depending on your data (watch for outliers). For these reasons\n",
    "  # we're going to use the StandardScaler. Get familiar with it by visiting\n",
    "  # these two websites:\n",
    "  #\n",
    "  # http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "  #\n",
    "  # http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "  #\n",
    "\n",
    "\n",
    "  # ---------\n",
    "  # Feature scaling is the type of transformation that only changes the\n",
    "  # scale and not number of features, so we'll use the original dataset\n",
    "  # column names. However we'll keep in mind that the _units_ have been\n",
    "    # altered:\n",
    "    scaled = preprocessing.StandardScaler().fit_transform(df)\n",
    "    scaled = pd.DataFrame(scaled, columns=df.columns)\n",
    "    print(\"New Variances:\\n\", scaled.var())\n",
    "    print(\"New Describe:\\n\", scaled.describe())\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def drawVectors(transformed_features, components_, columns, plt, scaled):\n",
    "  if not scaled:\n",
    "    return plt.axes() # No cheating ;-)\n",
    "\n",
    "  num_columns = len(columns)\n",
    "\n",
    "  # This funtion will project your *original* feature (columns)\n",
    "  # onto your principal component feature-space, so that you can\n",
    "  # visualize how \"important\" each one was in the\n",
    "  # multi-dimensional scaling\n",
    "  \n",
    "  # Scale the principal components by the max value in\n",
    "  # the transformed set belonging to that component\n",
    "  xvector = components_[0] * max(transformed_features[:,0])\n",
    "  yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "  ## visualize projections\n",
    "\n",
    "  # Sort each column by it's length. These are your *original*\n",
    "  # columns, not the principal components.\n",
    "  important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "  important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "  print(\"Features by importance:\\n\", important_features)\n",
    "\n",
    "  ax = plt.axes()\n",
    "\n",
    "  for i in range(num_columns):\n",
    "    # Use an arrow to project each original feature as a\n",
    "    # labeled vector on your principal component axes\n",
    "    plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75)\n",
    "    plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.75)\n",
    "\n",
    "  return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assignment3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assignment3.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import assignment2_helper as helper\n",
    "%matplotlib inline\n",
    "\n",
    "# Look pretty...\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Do * NOT * alter this line, until instructed!\n",
    "scaleFeatures = True\n",
    "\n",
    "\n",
    "# TODO: Load up the dataset and remove any and all\n",
    "# Rows that have a nan. You should be a pro at this\n",
    "# by now ;-)\n",
    "#\n",
    "# .. your code here ..\n",
    "kidneyData = pd.read_csv('Datasets/kidney_disease.csv')\n",
    "kidneyDataNull = kidneyData.dropna(axis = 0, how='any' ).reset_index(drop = True)\n",
    "\n",
    "# Create some color coded labels; the actual label feature\n",
    "# will be removed prior to executing PCA, since it's unsupervised.\n",
    "# You're only labeling by color so you can see the effects of PCA\n",
    "labels = ['red' if i=='ckd' else 'green' for i in kidneyDataNull.classification]\n",
    "\n",
    "\n",
    "# TODO: Use an indexer to select only the following columns:\n",
    "#       ['bgr','wc','rc']\n",
    "#\n",
    "# .. your code here ..\n",
    "\n",
    "#selectMain = kidneyDataNull.loc[:, ['bgr', 'wc', 'rc']]\n",
    "\n",
    "# TODO: Print out and check your dataframe's dtypes. You'll probably\n",
    "# want to call 'exit()' after you print it out so you can stop the\n",
    "# program's execution.\n",
    "#\n",
    "#print(selectMain.dtypes)\n",
    "\n",
    "\n",
    "# You can either take a look at the dataset webpage in the attribute info\n",
    "# section: https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease\n",
    "# or you can actually peek through the dataframe by printing a few rows.\n",
    "# What kind of data type should these three columns be? If Pandas didn't\n",
    "# properly detect and convert them to that data type for you, then use\n",
    "# an appropriate command to coerce these features into the right type.\n",
    "#\n",
    "# .. your code here ..\n",
    "\n",
    "#selectMain = selectMain.apply(lambda s: pd.to_numeric(s))\n",
    "#print(selectMain.dtypes)\n",
    "# TODO: PCA Operates based on variance. The variable with the greatest\n",
    "# variance will dominate. Go ahead and peek into your data using a\n",
    "# command that will check the variance of every feature in your dataset.\n",
    "\n",
    "#print(selectMain.var())\n",
    "# Print out the results. Also print out the results of running .describe\n",
    "# on your dataset.\n",
    "#The code here is when you drop the nominal features and run on only the numerical features\n",
    "#kidneyDataDrop = kidneyData.drop(['id', 'classification', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm',\\\n",
    "#                                  'cad', 'appet', 'pe', \\\n",
    "#                                  'ane'], axis = 1).dropna(axis = 0, how = 'any').reset_index(drop = True)\n",
    "#print(kidneyDataDrop.head())\n",
    "\n",
    "#selectMain = kidneyDataDrop.apply(lambda j: pd.to_numeric(j))\n",
    "\n",
    "#Trying out when the nominal values are there and you encode the nominal features \n",
    "kidneyDataNominal = kidneyData.drop(['id', 'classification'], axis = 1).dropna(axis = 0, how = 'any').reset_index(drop = True)\n",
    "selectMain = pd.get_dummies(kidneyDataNominal, columns= ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm',\\\n",
    "                                                         'cad', 'appet', 'pe', 'ane'] )\n",
    "print(selectMain.dtypes)\n",
    "selectMain = selectMain.apply(lambda m: pd.to_numeric(m))\n",
    "# Hint: If you don't see all three variables: 'bgr','wc' and 'rc', then\n",
    "# you probably didn't complete the previous step properly.\n",
    "#\n",
    "# .. your code here ..\n",
    "print(selectMain.describe())\n",
    "\n",
    "\n",
    "# TODO: This method assumes your dataframe is called df. If it isn't,\n",
    "# make the appropriate changes. Don't alter the code in scaleFeatures()\n",
    "# just yet though!\n",
    "#\n",
    "# .. your code adjustment here ..\n",
    "if scaleFeatures: selectMain = helper.scaleFeatures(selectMain)\n",
    "\n",
    "\n",
    "# TODO: Run PCA on your dataset and reduce it to 2 components\n",
    "# Ensure your PCA instance is saved in a variable called 'pca',\n",
    "# and that the results of your transformation are saved in 'T'.\n",
    "#\n",
    "# .. your code here ..\n",
    "    #import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(selectMain)\n",
    "T = pca.transform(selectMain)\n",
    "\n",
    "\n",
    "# Plot the transformed data as a scatter plot. Recall that transforming\n",
    "# the data will result in a NumPy NDArray. You can either use MatPlotLib\n",
    "# to graph it directly, or you can convert it to DataFrame and have pandas\n",
    "# do it for you.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.set_title('Unreduced Format of selectMain')\n",
    "ax.set_xlabel('bgr')\n",
    "ax.set_ylabel('rc')\n",
    "ax.set_zlabel('wc')\n",
    "ax.scatter(selectMain.bgr, selectMain.rc, selectMain.wc, c = 'green', marker = '.')\n",
    "#\n",
    "\n",
    "#plot reduced form of selectMain\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.set_title('PCA reduction of Selected Features of KidneyData')\n",
    "ax2.set_xlabel('Component 1')\n",
    "ax2.set_ylabel('Component 2')\n",
    "ax2.scatter(T[:, 0], T[:, 1], c = 'orange' , marker = 'o', alpha = 0.75)\n",
    "plt.show()\n",
    "# Since we've already demonstrated how to plot directly with MatPlotLib in\n",
    "# Module4/assignment1.py, this time we'll convert to a Pandas Dataframe.\n",
    "#\n",
    "# Since we transformed via PCA, we no longer have column names. We know we\n",
    "# are in P.C. space, so we'll just define the coordinates accordingly:\n",
    "ax = helper.drawVectors(T, pca.components_, selectMain.columns.values, plt, scaleFeatures)\n",
    "T = pd.DataFrame(T)\n",
    "T.columns = ['component1', 'component2']\n",
    "\n",
    "T.plot.scatter(x='component1', y='component2', marker='o', c=labels, alpha=0.75, ax=ax)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
